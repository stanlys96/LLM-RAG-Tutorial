from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Load environment variables
load_dotenv()

# Connect to your document database
persistent_directory = "db/chroma_db"
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
db = Chroma(persistent_directory=persistent_directory, embedding_function=embeddings)

# Set up AI model
model = ChatOpenAI(model="gpt-4o")

# Store our conversation as messages
chat_history = []

def ask_question(user_question):
  print(f"\n--- You asked: {user_question} ---")

  # Step 1: Make the question clear using conversation history
  if chat_history:
    # Ask AI to make the question standalone
    messages = [
      SystemMessage(content="Given the chat history, rewrite the new question to be standalone and searchable. Just return the rewritten question."),
    ] + chat_history + [
      HumanMessage(content=f"New question: {user_question}")
    ]

    result = model.invoke(messages)
    search_question = result.content.strip()
    print(f"Searching for {search_question}")
  else:
    search_question = user_question
  
  # Step 2: Find relevant documents
  retriever = db.as_retriever(search_kwargs={"k": 3})
  docs = retriever.invoke(search_question)

  print(f"Found {len(docs)} relevant documents:")
  for i, doc in enumerate(docs, 1):
    # Show first 2 lines of each document
    lines = doc.page_content.split('\n')[:2]
    preview = "\n".join(lines)
    print(f"Doc {i}: {preview}...")
  
  # Step 3: Create final prompt
  combined_input = f"""Based on the following documents, please answer this question: {user_question}

  Documents:
  {"\n".join([f"- {doc.page_content}" for doc in docs])}

  Please provide a clear, helpful answer using only the information from these documents. If you can't find the answer in the documents, say "I don't have enough information to answer that question based on the provided documents."
  """